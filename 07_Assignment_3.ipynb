{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Object recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Breakdown\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanics\n",
    "\n",
    "Fill out this notebook, save it, and submit it **and a `.txt` file** (see Part 2) in answer to this assignment.\n",
    "**You need to submit these two files electronically as described below.**\n",
    "\n",
    "On a DICE environment, open the terminal, navigate to the location of this notebook, and submit this notebook file using the following command:\n",
    "\n",
    "`submit iaml cw2 07_Assignment_3.ipynb assignment_3_predictions.txt`\n",
    "\n",
    "What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You can check the status of your submissions with the `show_submissions` command.\n",
    "\n",
    "**Distance Learners:** To copy your work up to DICE (such that you can use the `submit` command) you can use `scp` or `rsync` (you may need to install these yourself). You can copy files up using `student.ssh.inf.ed.ac.uk`, then ssh in to submit, e.g. (in a unix terminal):\n",
    "```\n",
    "filename1=07_Assignment_3.ipynb\n",
    "local_scp_filepath1=~/git/iaml2017/${filename1}\n",
    "filename2=assignment_3_predictions.txt\n",
    "local_scp_filepath2=~/git/iaml2017/${filename2}\n",
    "UUN=s0816700\n",
    "server_address=student.ssh.inf.ed.ac.uk\n",
    "scp -r ${local_scp_filepath1} ${UUN}@${server_address}:${filename1}\n",
    "scp -r ${local_scp_filepath2} ${UUN}@${server_address}:${filename2}\n",
    "# rsync -rl ${local_scp_filepath1} ${UUN}@${server_address}:${filename1}\n",
    "# rsync -rl ${local_scp_filepath2} ${UUN}@${server_address}:${filename2}\n",
    "ssh ${UUN}@${server_address}\n",
    "ssh student.login\n",
    "submit iaml cw2 07_Assignment_3.ipynb assignment_3_predictions.txt\n",
    "```\n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics MSc Degree Guide is that normally you will not be allowed to submit coursework late. See http://www.inf.ed.ac.uk/teaching/years/msc/courseguide10.html#exam for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you should NOT borrow actual text or code from other students. We ask that you provide a list of the people who you've had discussions with (if any).\n",
    "\n",
    "**Resubmission:** If you submit your file again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/JamesOwers/iaml2017) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate iaml\n",
    "cd iaml_2017\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers.\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (the `datasets` directory is adjacent to this file).\n",
    "\n",
    "1. **IMPORTANT:** Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Include all required imports and execute the cell below. It's typical to include package imports at the top of the file for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from operator import itemgetter\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the dataset\n",
    "In this assignment our goal is to recognize objects in images of realistic scenes. There are 19 different classes of object e.g. person, dog, cat, car, etc. The dataset derives from several thousands photographs harvested from the web. Each object of a relevant class has been manually annotated with a bounding box. Images can contain none, one or multiple objects of each class. We have prepared a [website](http://www.inf.ed.ac.uk/teaching/courses/iaml/2014/assts/asst3/images.html) where you can view the images.\n",
    "\n",
    "We are going to detect whether images contain a person or not - a binary classification problem. To save you time and to make the problem manageable with limited computational resources, we have preprocessed the dataset. We will use the [Bag of Visual Words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision) representation. Each column of the dataset (which is not an lable), refers to a 'visual word'. Each image is represented by a 500 dimensional vector that contains the normalized count for each of 500 different visual words present in the respective image (a similar representation is used for the spambase dataset, just for real words). See the Appendix at the bottom of the notebook for more information. The image data is thus a $N \\times 500$ dimensional matrix where `N` is the number of images.\n",
    "\n",
    "The full dataset has 520 attributes (dimensions). The first attribute (`imgID`) contains the image ID which allows you to associate a data point with an actual image. The next 500 attributes (`dim1`, ..., `dim500`) are a normalized count vector for each visual word. The last 19 attributes (`is_class`) are the labels - 1 means the class is present in the image. In most of the experiments (unless explicitly noted otherwise) you will only need the `is_person` attribute and the 500 dimensional feature vector. **Do not use the additional class indicator attributes as features** unless explicitly told to do so. \n",
    "\n",
    "In Part A we provide you with a training (`train_images partA.csv`) and a validation (`valid_images partA .csv`) dataset. In Part B we provide three data sets: a training set (`train_images partB.csv`), a validation set (`valid_images partB.csv`), and a test set (`test_images partB.csv`). The training and validation set contain valid labels. In the test set the labels are missing. The files are available from the GitHub repository. \n",
    "\n",
    "**Important**: *Throughout the assignment you will be given various versions of the dataset that are relevant\n",
    "to a particular question. Please be careful to use the correct version of the dataset when instructed to do so.\n",
    "If you use the wrong version of the dataset by mistake no marks will be awarded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploration of the dataset [70%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 --- [1 mark] ==========\n",
    "Create two Pandas DataFrame objects called `train_A` and `valid_A` by loading the datasets `train_images_partA.csv` and `valid_images_partA.csv`. Display the number of data points and attributes in each of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "train_A = pd.read_csv('datasets/train_images_partA.csv')\n",
    "valid_A = pd.read_csv('datasets/valid_images_partA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 --- [1 mark] ==========\n",
    "Display and inspect the first 10 instances in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>...</th>\n",
       "      <th>is_cow</th>\n",
       "      <th>is_diningtable</th>\n",
       "      <th>is_dog</th>\n",
       "      <th>is_horse</th>\n",
       "      <th>is_motorbike</th>\n",
       "      <th>is_person</th>\n",
       "      <th>is_pottedplant</th>\n",
       "      <th>is_sheep</th>\n",
       "      <th>is_sofa</th>\n",
       "      <th>is_tvmonitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008_000008</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008_000015</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008_000019</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008_000023</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008_000028</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008_000033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008_000036</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008_000037</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008_000041</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008_000045</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         imgId      dim1      dim2      dim3      dim4      dim5      dim6  \\\n",
       "0  2008_000008  0.002232  0.000558  0.002790  0.000837  0.001674  0.001953   \n",
       "1  2008_000015  0.001563  0.000391  0.007422  0.003516  0.003906  0.005078   \n",
       "2  2008_000019  0.000521  0.000000  0.000000  0.001042  0.001563  0.005729   \n",
       "3  2008_000023  0.002976  0.002232  0.004464  0.000372  0.000372  0.002232   \n",
       "4  2008_000028  0.001359  0.000340  0.001359  0.000340  0.001359  0.002038   \n",
       "5  2008_000033  0.000000  0.006324  0.000372  0.000372  0.000372  0.000372   \n",
       "6  2008_000036  0.000340  0.000000  0.004416  0.000340  0.000679  0.006114   \n",
       "7  2008_000037  0.000837  0.002232  0.000279  0.000279  0.000837  0.000000   \n",
       "8  2008_000041  0.002378  0.001359  0.004755  0.001019  0.003736  0.001359   \n",
       "9  2008_000045  0.001019  0.000340  0.006454  0.001698  0.001359  0.003736   \n",
       "\n",
       "       dim7      dim8      dim9      ...       is_cow  is_diningtable  is_dog  \\\n",
       "0  0.001395  0.002232  0.003627      ...            0               0       0   \n",
       "1  0.001953  0.002344  0.001953      ...            0               0       0   \n",
       "2  0.000521  0.002083  0.003646      ...            0               0       1   \n",
       "3  0.000000  0.003720  0.000000      ...            0               0       0   \n",
       "4  0.002378  0.000000  0.003397      ...            0               0       0   \n",
       "5  0.000744  0.008185  0.000372      ...            0               0       0   \n",
       "6  0.001359  0.002717  0.003057      ...            0               0       0   \n",
       "7  0.000279  0.006696  0.000000      ...            0               0       0   \n",
       "8  0.001019  0.004076  0.003397      ...            0               1       0   \n",
       "9  0.000000  0.004076  0.000000      ...            0               0       0   \n",
       "\n",
       "   is_horse  is_motorbike  is_person  is_pottedplant  is_sheep  is_sofa  \\\n",
       "0         1             0          1               0         0        0   \n",
       "1         0             0          0               0         0        0   \n",
       "2         0             0          0               0         0        0   \n",
       "3         0             0          1               0         0        0   \n",
       "4         0             0          0               0         0        0   \n",
       "5         0             0          0               0         0        0   \n",
       "6         0             0          1               0         0        0   \n",
       "7         0             0          0               0         0        0   \n",
       "8         0             0          1               0         0        0   \n",
       "9         0             0          0               0         0        0   \n",
       "\n",
       "   is_tvmonitor  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  \n",
       "5             0  \n",
       "6             0  \n",
       "7             0  \n",
       "8             0  \n",
       "9             0  \n",
       "\n",
       "[10 rows x 520 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 10 training instances\n",
    "train_A.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 --- [1 mark] ==========\n",
    "Select the attributes (i.e. input features) for training our classifiers. These should be the visual word normalised counts `dim1, dim2, ..., dim500`. Create a list of the **names** of the attributes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imgId', 'dim1', 'dim2', 'dim3', 'dim4', 'dim5', 'dim6', 'dim7', 'dim8', 'dim9', 'dim10', 'dim11', 'dim12', 'dim13', 'dim14', 'dim15', 'dim16', 'dim17', 'dim18', 'dim19', 'dim20', 'dim21', 'dim22', 'dim23', 'dim24', 'dim25', 'dim26', 'dim27', 'dim28', 'dim29', 'dim30', 'dim31', 'dim32', 'dim33', 'dim34', 'dim35', 'dim36', 'dim37', 'dim38', 'dim39', 'dim40', 'dim41', 'dim42', 'dim43', 'dim44', 'dim45', 'dim46', 'dim47', 'dim48', 'dim49', 'dim50', 'dim51', 'dim52', 'dim53', 'dim54', 'dim55', 'dim56', 'dim57', 'dim58', 'dim59', 'dim60', 'dim61', 'dim62', 'dim63', 'dim64', 'dim65', 'dim66', 'dim67', 'dim68', 'dim69', 'dim70', 'dim71', 'dim72', 'dim73', 'dim74', 'dim75', 'dim76', 'dim77', 'dim78', 'dim79', 'dim80', 'dim81', 'dim82', 'dim83', 'dim84', 'dim85', 'dim86', 'dim87', 'dim88', 'dim89', 'dim90', 'dim91', 'dim92', 'dim93', 'dim94', 'dim95', 'dim96', 'dim97', 'dim98', 'dim99', 'dim100', 'dim101', 'dim102', 'dim103', 'dim104', 'dim105', 'dim106', 'dim107', 'dim108', 'dim109', 'dim110', 'dim111', 'dim112', 'dim113', 'dim114', 'dim115', 'dim116', 'dim117', 'dim118', 'dim119', 'dim120', 'dim121', 'dim122', 'dim123', 'dim124', 'dim125', 'dim126', 'dim127', 'dim128', 'dim129', 'dim130', 'dim131', 'dim132', 'dim133', 'dim134', 'dim135', 'dim136', 'dim137', 'dim138', 'dim139', 'dim140', 'dim141', 'dim142', 'dim143', 'dim144', 'dim145', 'dim146', 'dim147', 'dim148', 'dim149', 'dim150', 'dim151', 'dim152', 'dim153', 'dim154', 'dim155', 'dim156', 'dim157', 'dim158', 'dim159', 'dim160', 'dim161', 'dim162', 'dim163', 'dim164', 'dim165', 'dim166', 'dim167', 'dim168', 'dim169', 'dim170', 'dim171', 'dim172', 'dim173', 'dim174', 'dim175', 'dim176', 'dim177', 'dim178', 'dim179', 'dim180', 'dim181', 'dim182', 'dim183', 'dim184', 'dim185', 'dim186', 'dim187', 'dim188', 'dim189', 'dim190', 'dim191', 'dim192', 'dim193', 'dim194', 'dim195', 'dim196', 'dim197', 'dim198', 'dim199', 'dim200', 'dim201', 'dim202', 'dim203', 'dim204', 'dim205', 'dim206', 'dim207', 'dim208', 'dim209', 'dim210', 'dim211', 'dim212', 'dim213', 'dim214', 'dim215', 'dim216', 'dim217', 'dim218', 'dim219', 'dim220', 'dim221', 'dim222', 'dim223', 'dim224', 'dim225', 'dim226', 'dim227', 'dim228', 'dim229', 'dim230', 'dim231', 'dim232', 'dim233', 'dim234', 'dim235', 'dim236', 'dim237', 'dim238', 'dim239', 'dim240', 'dim241', 'dim242', 'dim243', 'dim244', 'dim245', 'dim246', 'dim247', 'dim248', 'dim249', 'dim250', 'dim251', 'dim252', 'dim253', 'dim254', 'dim255', 'dim256', 'dim257', 'dim258', 'dim259', 'dim260', 'dim261', 'dim262', 'dim263', 'dim264', 'dim265', 'dim266', 'dim267', 'dim268', 'dim269', 'dim270', 'dim271', 'dim272', 'dim273', 'dim274', 'dim275', 'dim276', 'dim277', 'dim278', 'dim279', 'dim280', 'dim281', 'dim282', 'dim283', 'dim284', 'dim285', 'dim286', 'dim287', 'dim288', 'dim289', 'dim290', 'dim291', 'dim292', 'dim293', 'dim294', 'dim295', 'dim296', 'dim297', 'dim298', 'dim299', 'dim300', 'dim301', 'dim302', 'dim303', 'dim304', 'dim305', 'dim306', 'dim307', 'dim308', 'dim309', 'dim310', 'dim311', 'dim312', 'dim313', 'dim314', 'dim315', 'dim316', 'dim317', 'dim318', 'dim319', 'dim320', 'dim321', 'dim322', 'dim323', 'dim324', 'dim325', 'dim326', 'dim327', 'dim328', 'dim329', 'dim330', 'dim331', 'dim332', 'dim333', 'dim334', 'dim335', 'dim336', 'dim337', 'dim338', 'dim339', 'dim340', 'dim341', 'dim342', 'dim343', 'dim344', 'dim345', 'dim346', 'dim347', 'dim348', 'dim349', 'dim350', 'dim351', 'dim352', 'dim353', 'dim354', 'dim355', 'dim356', 'dim357', 'dim358', 'dim359', 'dim360', 'dim361', 'dim362', 'dim363', 'dim364', 'dim365', 'dim366', 'dim367', 'dim368', 'dim369', 'dim370', 'dim371', 'dim372', 'dim373', 'dim374', 'dim375', 'dim376', 'dim377', 'dim378', 'dim379', 'dim380', 'dim381', 'dim382', 'dim383', 'dim384', 'dim385', 'dim386', 'dim387', 'dim388', 'dim389', 'dim390', 'dim391', 'dim392', 'dim393', 'dim394', 'dim395', 'dim396', 'dim397', 'dim398', 'dim399', 'dim400', 'dim401', 'dim402', 'dim403', 'dim404', 'dim405', 'dim406', 'dim407', 'dim408', 'dim409', 'dim410', 'dim411', 'dim412', 'dim413', 'dim414', 'dim415', 'dim416', 'dim417', 'dim418', 'dim419', 'dim420', 'dim421', 'dim422', 'dim423', 'dim424', 'dim425', 'dim426', 'dim427', 'dim428', 'dim429', 'dim430', 'dim431', 'dim432', 'dim433', 'dim434', 'dim435', 'dim436', 'dim437', 'dim438', 'dim439', 'dim440', 'dim441', 'dim442', 'dim443', 'dim444', 'dim445', 'dim446', 'dim447', 'dim448', 'dim449', 'dim450', 'dim451', 'dim452', 'dim453', 'dim454', 'dim455', 'dim456', 'dim457', 'dim458', 'dim459', 'dim460', 'dim461', 'dim462', 'dim463', 'dim464', 'dim465', 'dim466', 'dim467', 'dim468', 'dim469', 'dim470', 'dim471', 'dim472', 'dim473', 'dim474', 'dim475', 'dim476', 'dim477', 'dim478', 'dim479', 'dim480', 'dim481', 'dim482', 'dim483', 'dim484', 'dim485', 'dim486', 'dim487', 'dim488', 'dim489', 'dim490', 'dim491', 'dim492', 'dim493', 'dim494', 'dim495', 'dim496', 'dim497', 'dim498', 'dim499', 'dim500', 'is_aeroplane', 'is_bicycle', 'is_bird', 'is_boat', 'is_bottle', 'is_bus', 'is_car', 'is_cat', 'is_chair', 'is_cow', 'is_diningtable', 'is_dog', 'is_horse', 'is_motorbike', 'is_person', 'is_pottedplant', 'is_sheep', 'is_sofa', 'is_tvmonitor']\n"
     ]
    }
   ],
   "source": [
    "# Create list of attributes\n",
    "attribute_names = list(train_A.columns.values)\n",
    "print(attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4 --- [1 mark] ==========\n",
    "By using the list from Question 1.3 now create 4 numpy arrays `X_tr`, `X_val`, `y_tr` and `y_val` in which to store the training features, validation features, training targets, and validation targets, respectively. Your target vectors should correspond to the `is_person` attribute of the training and validation sets. Display the dimensionalities (i.e shapes) of the 4 arrays. \n",
    "\n",
    "Check this carefully - you will be penalised in following questions if the data is not correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr: (2093, 500)\n",
      "y_tr: (2093,)\n",
      "X_val: (1113, 500)\n",
      "y_val: (1113,)\n"
     ]
    }
   ],
   "source": [
    "# Setup training data\n",
    "X_tr = train_A.copy().drop(attribute_names[-19:], axis=1).drop('imgId', axis=1)\n",
    "y_tr = train_A.copy()['is_person']\n",
    "# Setup validation data\n",
    "X_val = valid_A.copy().drop(attribute_names[-19:], axis=1).drop('imgId', axis=1)\n",
    "y_val = valid_A.copy()['is_person']\n",
    "print('X_tr:', X_tr.shape)\n",
    "print('y_tr:', y_tr.shape)\n",
    "print('X_val:', X_val.shape)\n",
    "print('y_val:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 --- [2 marks] ==========\n",
    "Create a [countplots](https://seaborn.github.io/generated/seaborn.countplot.html?highlight=countplot#seaborn.countplot) for the training and validation targets. Create a single figure, and put the two plots inside the single figure. Label axes appropriately and add a title to your plot. Use descriptive `xticklabels` instead of the default numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAETCAYAAADaqxQ+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee1aaa668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n",
    "sns.countplot(x=y_tr, ax=ax1)\n",
    "sns.countplot(x=y_val, ax=ax2)\n",
    "ax1.set_title('Training Data')\n",
    "ax2.set_title('Validation Data')\n",
    "ax1.set_xticklabels(['Person', 'Not Person'])\n",
    "ax2.set_xticklabels(['Person', 'Not Person'])\n",
    "ax1.set_xlabel('Image Class')# Your code goes here\n",
    "ax2.set_xlabel('Image Class')\n",
    "ax1.set_ylabel('Number of Images')\n",
    "ax2.set_ylabel('Number of Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.6 --- [1 mark] ==========\n",
    "We want to make a baseline classification accuracy to beat. Pick a baseline \"dummy\" classifier, describe in a sentence why you chose it, and report the accuracy it achieves on the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple baseline classifier is just one that predicts the most common class every time. It's a good choice because any classifier that does worse than this should definitely not be used and it's an incredibly simple algorithm to implement yet achieves a relatively good accuracy of 52.7% on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 52.6504941599 %\n"
     ]
    }
   ],
   "source": [
    "total_data_points = y_tr.shape[0]\n",
    "num_person = np.sum(y_tr)\n",
    "num_not_person = total_data_points - num_person\n",
    "most_common_class = np.argmax([num_not_person, num_person])\n",
    "baseline_accuracy = (most_common_class == y_val).sum()/y_val.shape[0]\n",
    "print('Baseline Accuracy:',baseline_accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.7 --- [3 marks] ==========\n",
    "Train a [`LogisticRegression`](http://scikit-learn.org/0.17/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier using default settings, except for the `solver` parameter which you should set to `lbfgs`. Report the classification accuracy score on the training and validation sets and compare with the baseline. Comment on the results with 1-2 sentences. You may include any additional plot(s) if you wish to justify your explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 55.08838987099857%\r\n",
      "Accuracy Validation: 52.650494159928115%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X=X_tr, y=y_tr)\n",
    "lr_pred = lr.predict(X_val)\n",
    "accuracy_training = (lr.predict(X_tr) == y_tr).sum()/y_tr.shape[0]\n",
    "accuracy_validation = (lr_pred == y_val).sum()/y_val.shape[0]\n",
    "print('Accuracy Training: {}%\\r\\nAccuracy Validation: {}%'.format(accuracy_training*100, accuracy_validation*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the logistic regression on the validation set is 52.7%, which is not as much better than the baseline as I would have expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.8 --- [1 mark] ==========\n",
    "Display the means and standard deviations of the first 5 features in the training set. *Hint: you want to compute the means and standard deviations for each column in your arrays. Make sure you make appropriate use of the `axis` parameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean first 5 features: [ 0.00175058  0.00075592  0.00431695  0.00185278  0.00227248]\n",
      "Standard Deviation first 5 features: [ 0.00119281  0.00140604  0.00369255  0.00134017  0.00159773]\n"
     ]
    }
   ],
   "source": [
    "data_mean = np.mean(X_tr, axis=0)\n",
    "data_stdDev = np.std(X_tr, axis=0)\n",
    "print('Mean first 5 features: {}'.format(data_mean[:5].values))\n",
    "print('Standard Deviation first 5 features: {}'.format(data_stdDev[:5].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.9 --- [3 marks] ==========\n",
    "Feature standardisation is a pre-processing technique used to transform data so that they have zero mean and unit standard deviation. For many algorithms, this is a very important step for training models (both regression and classification). Read about [feature standardisation](http://scikit-learn.org/0.17/modules/preprocessing.html) and make sure you understand what kind of transformation this method applies to the data.\n",
    "\n",
    "`Scikit-learn` offers a [class](http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.StandardScaler.html) for feature standardisation. Instansiate a StandardScaler object and fit it using the **training features**. Use this fitted object to transform both your training and validation features to have a standard scale. \n",
    "\n",
    "Once your training and validation input data have been transformed, display the means and standard deviations of the first 5 attributes for **both** the training and validation sets. Are the results as you expected? Explain your answer in 2-3 sentences. Why didn't we use the validation set to standardise the data?\n",
    "\n",
    "**IMPORTANT: You should use the transformed data for the rest of this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean first 5 features: [ -3.39485301e-17  -2.20665446e-17   2.44429417e-16   1.42583827e-16\n",
      "  -5.77125012e-17]\n",
      "Standard Deviation first 5 features: [ 1.01310886  0.96981949  1.0492285   1.05684479  0.97471336]\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "sc = sc.fit(X_tr)\n",
    "transf_x_tr = sc.transform(X_tr)\n",
    "transf_x_val = sc.transform(X_val)\n",
    "data_mean_transf = np.mean(transf_x_tr, axis=0)\n",
    "data_stdDev_transf = np.std(transf_x_val, axis=0)\n",
    "print('Mean first 5 features: {}'.format(data_mean_transf[:5]))\n",
    "print('Standard Deviation first 5 features: {}'.format(data_stdDev_transf[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers are about what I would expect as each mean is very nearly 0 and the standard deviations are all very close to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.10 --- [3 marks] ==========\n",
    "By using the transformed input data, train a new `LogisticRegression` classifier. Again, set the `solver` parameter to `lbfgs` and use default settings for the other parameters. Report the classification accuracy on both the training and validation sets.\n",
    "\n",
    "Comment on how your model compares to the baseline classifier from Question 1.6? You may use additional plot(s) to support your explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training: 65.12183468705209%\n",
      "Accuracy Validation: 64.33063791554358%\n"
     ]
    }
   ],
   "source": [
    "lr_transf = LogisticRegression(solver='lbfgs')\n",
    "lr_transf = lr_transf.fit(transf_x_tr, y_tr)\n",
    "lr_transf_pred = lr_transf.predict(transf_x_val)\n",
    "accuracy_training = (lr.predict(transf_x_tr) == y_tr).sum()/y_tr.shape[0]\n",
    "accuracy_validation = (lr_transf_pred == y_val).sum()/y_val.shape[0]\n",
    "print('Accuracy Training: {}%\\r\\nAccuracy Validation: {}%'.format(accuracy_training*100, accuracy_validation*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data significanctly improved the performance of the logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.11 --- [1 mark] ==========\n",
    "So far we have used default settings for training the logistic regression classifier. Now, we want to use [K-fold cross-validation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to optimise the regularisation parameter `C`. The regularisation parameter controls the degree to which we wish to penalise large magnitudes in the weight vector. This can help us prevent overfitting but, if set too high, could lead us to underfit too.\n",
    "\n",
    "Create a 3-fold cross-validation object. Set the `shuffle` parameter to `True` and the `random_state` to `0`. By using the cross-validation iterator, display the number of test samples for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size per fold: [ 698.  698.  697.]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "i = 0\n",
    "fold_test_size = np.zeros(3)\n",
    "for train, test in kf.split(transf_x_tr):\n",
    "    fold_test_size[i] = test.shape[0]\n",
    "    i+=1\n",
    "print('Test size per fold:', fold_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.12 --- [2 marks] ========== \n",
    "Using the cross-validation iterator from the previous question, perform a search for the best value for `C`. \n",
    "\n",
    "We are going to loop over each CV fold, and each value of `C`. The values of `C` to search should be 20 equally-spaced values **in log space** ranging from `1e-5` to `1e5` *(hint: look at the `logspace()` function in numpy)*. \n",
    "\n",
    "Create a 2-dimensional array and, for each cross-validation fold and parameter setting pair, compute and store the classification accuracy score e.g. store the score of fold 0 with parameter setting 1 at score_array[0,1]. As previously, set the `solver` parameter to `lbfgs` and use default settings for the other parameters (except for `C` obviously!).\n",
    "\n",
    "*(hint: you could use two loops in your code; one iterating over CV folds and another one iterating over the values for `C`)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5730659   0.62893983  0.66762178  0.67191977  0.68767908  0.69627507\n",
      "   0.6747851   0.67048711  0.65329513  0.65902579  0.65902579  0.65329513\n",
      "   0.64899713  0.64613181  0.64613181  0.64899713  0.64613181  0.64613181\n",
      "   0.64899713  0.64899713]\n",
      " [ 0.54584527  0.63180516  0.69484241  0.71919771  0.72922636  0.73495702\n",
      "   0.70200573  0.6747851   0.65472779  0.63610315  0.63180516  0.63323782\n",
      "   0.6260745   0.62750716  0.62750716  0.6260745   0.6260745   0.6260745\n",
      "   0.62750716  0.6260745 ]\n",
      " [ 0.54806313  0.63558106  0.64275466  0.65566714  0.68005739  0.70014347\n",
      "   0.68149211  0.67001435  0.66284075  0.64418938  0.63271162  0.62553802\n",
      "   0.6241033   0.6241033   0.6241033   0.6241033   0.6241033   0.6241033\n",
      "   0.6241033   0.6241033 ]]\n"
     ]
    }
   ],
   "source": [
    "search_space_c = np.logspace(-5, 5, num=20)\n",
    "accuracy_values = np.zeros((3, search_space_c.shape[0]))\n",
    "for i in range(search_space_c.shape[0]):\n",
    "    lr_cv = LogisticRegression(solver='lbfgs', C=search_space_c[i])\n",
    "    current_fold_num = 0\n",
    "    for train, test in kf.split(transf_x_tr):\n",
    "        lr_cv = lr_cv.fit(transf_x_tr[train], y_tr[train])\n",
    "        pred = lr_cv.predict(transf_x_tr[test])\n",
    "        accuracy = (pred == y_tr[test]).sum()/y_tr[test].shape[0]\n",
    "        accuracy_values[current_fold_num, i] = accuracy\n",
    "        current_fold_num += 1\n",
    "print(accuracy_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Question 1.13 --- [1 mark] ========== \n",
    "Plot the mean classification performance (across CV folds) of the logistic regression classifier against the regularisation parameter `C` by using the range from Question 1.12. Use a logarithmic scale for the x-axis and label both axes appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5556581   0.63210868  0.66840628  0.68226154  0.69898761  0.71045852\n",
      "  0.68609431  0.67176218  0.65695456  0.64643944  0.64118086  0.63735699\n",
      "  0.63305831  0.63258076  0.63258076  0.63305831  0.6321032   0.6321032\n",
      "  0.63353587  0.63305831]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFlCAYAAADs50HhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfQY+MSb4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee2b66828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_mean = np.mean(accuracy_values, axis=0)\n",
    "plt.scatter(np.log(search_space_c), fold_mean)\n",
    "plt.xlabel('Log C Value')\n",
    "plt.ylabel('Mean Classifier Accuracy')\n",
    "plt.title('Mean Accuracy vs Log C Value')\n",
    "print(fold_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.14 --- [2 marks] ==========\n",
    "Display the optimal value for the regularisation parameter `C` determined by the search results from Question 1.12. Similarly to Question 1.13, consider the mean classifiation accuracy across CV folds. By using the optimal value (i.e. the one that yields the highest average classification accuracy) train a new `LogisticRegression` classifier and report the classification accuracy on the validation set. *(Hint: Do not pick the optimal value \"by hand\", instead use an appropriate numpy function).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C Value: 0.004281332398719391\n",
      "Accuracy: 69.63162623539982%\n"
     ]
    }
   ],
   "source": [
    "optimal_c_index = np.argmax(fold_mean)\n",
    "optimal_c_value = search_space_c[optimal_c_index]\n",
    "print(\"Optimal C Value: {}\".format(optimal_c_value))\n",
    "# Train a new logistic regression\n",
    "optimal_lr = LogisticRegression(solver='lbfgs', C=optimal_c_value)\n",
    "optimal_lr = optimal_lr.fit(transf_x_tr, y_tr)\n",
    "pred_optimal_lr = optimal_lr.predict(transf_x_val)\n",
    "accuracy = (pred_optimal_lr == y_val).sum()/y_val.shape[0]\n",
    "print(\"Accuracy: {}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.15 --- [1 mark] ========== \n",
    "Scikit-learn offers a [`LogisticRegressionCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) module which implements Logistic Regression with builtin cross-validation to find out the optimal `C` parameter. You can specify the range for the `C` parameter, as well as the cross-validation method you want to use with the `Cs` and `cv` parameters, respectively. Use the `C` range you set up in Question 1.12 and the 3-fold cross-validation iterator from Question 1.11. Once again, train the models by using the `lbfgs` optimisation method and display the optimal value for the parameter `C`. Finally, display the classification accuracy on the validation set. Check your results are consistent with those from Question 1.14!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Value: 0.004281332398719391\n",
      "Accuracy: 69.63162623539982%\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_cv = LogisticRegressionCV(solver='lbfgs', Cs=search_space_c, cv=kf)\n",
    "logistic_regression_cv.fit(transf_x_tr, y_tr)\n",
    "pred_lr_cv = optimal_lr.predict(transf_x_val)\n",
    "print(\"C Value: {}\".format(logistic_regression_cv.C_[0]))\n",
    "accuracy = (pred_lr_cv == y_val).sum()/y_val.shape[0]\n",
    "print(\"Accuracy: {}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.16 --- [1 mark] ==========\n",
    "Now, we want to validate the importance of various features for classification. For this purpose, we will use a [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) (you might want to refer to the labs if you are unsure how we can estimate feature importances with decision tree and random forest models).\n",
    "\n",
    "Initialise a random forest classifier and fit the model by using training data only and 500 trees (i.e. `n_estimators`). Set the `RandomState` equal to 42 to ensure reproducible results. Report the accuracy score on both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0%\n",
      "Accuracy: 69.2722371967655%\n"
     ]
    }
   ],
   "source": [
    "transf_x_train, transf_x_test, y_train, y_test = train_test_split(transf_x_tr, y_tr, test_size=.2, random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf_classifier.fit(transf_x_tr, y_tr)\n",
    "pred_rf_training = rf_classifier.predict(transf_x_tr)\n",
    "pred_rf = rf_classifier.predict(transf_x_val)\n",
    "accuracy = (pred_rf == y_val).sum()/y_val.shape[0]\n",
    "accuracy_tr = (pred_rf_training == y_tr).sum()/y_tr.shape[0]\n",
    "print(\"Training Accuracy: {}%\".format(accuracy_tr*100))\n",
    "print(\"Accuracy: {}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.17 --- [2 marks] ==========\n",
    "Comment on the results above. Do you find the discrepancy between training and validation accuracies surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.18 --- [2 marks] ==========\n",
    "By using the random forest model from the previous question order the features by descending importance and display the names of the 50 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dim20', 0.0075719505701260965), ('dim50', 0.0075594322149745204), ('dim359', 0.0070315402883795704), ('dim3', 0.0061929705370030591), ('dim478', 0.0060540675045530276), ('dim75', 0.0060262086838827065), ('dim282', 0.0058105619577742519), ('dim460', 0.0056058489053563308), ('dim342', 0.0054513322650658197), ('dim397', 0.0048591610911170079), ('dim347', 0.0047832305242377221), ('dim73', 0.0047419278129818138), ('dim484', 0.0047160452789987159), ('dim355', 0.0047022181382774964), ('dim329', 0.0046400953650727866), ('dim216', 0.0046058375787848471), ('dim262', 0.004577404157967961), ('dim221', 0.0042181328053523719), ('dim133', 0.0041313654581943008), ('dim499', 0.0040637948024201979), ('dim273', 0.0040416252061222626), ('dim253', 0.0040019579222058243), ('dim205', 0.0038899958735214781), ('dim422', 0.0038673852895302475), ('dim346', 0.0038130599781742995), ('dim16', 0.0037277779229481956), ('dim325', 0.0037043236955780121), ('dim89', 0.003703172661177383), ('dim91', 0.0037007988101297093), ('dim328', 0.003687210362649383), ('dim213', 0.0036676459142687113), ('dim321', 0.0036134338766544553), ('dim288', 0.003595346912535817), ('dim53', 0.0035909587517429374), ('dim439', 0.0035810213699269018), ('dim426', 0.0035761205471165495), ('dim72', 0.0034617930464178734), ('dim34', 0.0034613280076354314), ('dim76', 0.0034144246543663707), ('dim311', 0.0033631172637428029), ('dim314', 0.0033369665831289438), ('dim95', 0.0033122293758115662), ('dim287', 0.0033080171115574459), ('dim482', 0.0032585557535926954), ('dim298', 0.0032508807562648353), ('dim47', 0.0032058471514611947), ('dim441', 0.0031713532983600485), ('dim200', 0.0030561034829098186), ('dim343', 0.0030115927967987893), ('dim293', 0.0030056350037862005)]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = zip(attribute_names, rf_classifier.feature_importances_, )\n",
    "feature_importance = sorted(feature_importance, key=itemgetter(1))[::-1]\n",
    "top_50_features = feature_importance[:50]\n",
    "print(top_50_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.19 --- [3 marks] ==========\n",
    "Next, we would like to test the performance of support vector machines. Train three support vector classifiers with the following kernels: linear, radial basis function, and polynomial. Report the classification accuracy of each of the three classifiers on both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 65.63245823389022% Testing Accuracy: 64.33063791554358% - Linear\n",
      "Training Accuracy: 71.59904534606206% Testing Accuracy: 71.96765498652292% - Radial Basis\n",
      "Training Accuracy: 71.1217183770883% Testing Accuracy: 69.90116801437556% - Polynomial\n"
     ]
    }
   ],
   "source": [
    "# Create 3 SVMs\n",
    "svm_linear = svm.LinearSVC()\n",
    "svm_radial_basis = svm.SVC(kernel='rbf')\n",
    "svm_polynomial = svm.SVC(kernel='poly')\n",
    "# Train the SVMs\n",
    "svm_linear.fit(transf_x_train, y_train)\n",
    "svm_radial_basis.fit(transf_x_train, y_train)\n",
    "svm_polynomial.fit(transf_x_train, y_train)\n",
    "# Calculate accuracy\n",
    "svm_linear_accuracy_train = (svm_linear.predict(transf_x_test) == y_test).sum()/y_test.shape[0] * 100\n",
    "svm_linear_accuracy_val = (svm_linear.predict(transf_x_val) == y_val).sum()/y_val.shape[0] * 100\n",
    "svm_radial_basis_accuracy_train = (svm_radial_basis.predict(transf_x_test) == y_test).sum()/y_test.shape[0] * 100\n",
    "svm_radial_basis_accuracy_val = (svm_radial_basis.predict(transf_x_val) == y_val).sum()/y_val.shape[0] * 100\n",
    "svm_polynomial_accuracy_train = (svm_polynomial.predict(transf_x_test) == y_test).sum()/y_test.shape[0] * 100\n",
    "svm_polynomial_accuracy_val = (svm_polynomial.predict(transf_x_val) == y_val).sum()/y_val.shape[0] * 100\n",
    "# Display accuracy for each classifier\n",
    "print(\"Training Accuracy: {}% Testing Accuracy: {}% - Linear\".format(svm_linear_accuracy_train, svm_linear_accuracy_val))\n",
    "print(\"Training Accuracy: {}% Testing Accuracy: {}% - Radial Basis\".format(svm_radial_basis_accuracy_train, svm_radial_basis_accuracy_val))\n",
    "print(\"Training Accuracy: {}% Testing Accuracy: {}% - Polynomial\".format(svm_polynomial_accuracy_train, svm_polynomial_accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.20 --- [3 marks] ==========\n",
    "At this point, we would like to get an idea of what kind of decision boundaries we can get with the three types of SVM kernels we introduced in the previous question. For visualisation, however, we can only make use of 2D input data. For this purpose, we select to use the 21st and 76th columns of our training features (*hint: remember that Python uses 0-based indexing*). \n",
    "\n",
    "Execute the cell below to define a useful function which we will be using to plot the decision boundaries *(it is also not a bad idea to try to understand what this functions does)*. \n",
    "\n",
    "Then train three distinct SVM classifiers by using the 2D input data mentioned above and default parameters:\n",
    "* a linear SVC\n",
    "* an RBF SVC \n",
    "* a polynomial SVC\n",
    "\n",
    "Finally, create a list containing the three classifiers you have just trained. Use this list as an input to the provided function along with the used training features and observe the outcome. You can use the additional `title` parameter to set the titles in the subplots. Comment on the shape of the boundaries and what this means for classification accuracy in 1-2 sentences.\n",
    "\n",
    "*(Acknowledgement: this Question has been heavily based on [this example](http://scikit-learn.org/0.17/auto_examples/svm/plot_iris.html) from scikit-learn's documentation.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_svc_decision_boundaries(clfs, X, title=None):\n",
    "    \"\"\"Plots decision boundaries for classifiers with 2D inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : list\n",
    "        Classifiers for which decision boundaries will be displayed.\n",
    "    X : array\n",
    "        Input features used to train the classifiers.\n",
    "    title : list, optional\n",
    "        Titles for classifiers.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert X.shape[1] == 2 # Input should be 2D\n",
    "    if title is not None:\n",
    "        assert len(clfs) == len(title)\n",
    "    \n",
    "    h = .04 # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for i, clf in enumerate(clfs):\n",
    "        plt.subplot(1, len(clfs), i + 1)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "        # Training points\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y_tr, cmap=plt.cm.Paired)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        if title is not None:\n",
    "            plt.title(title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid RGBA argument: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Not in cache, or unhashable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (1, None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-7313cfe15db2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mselect_svm_radial_basis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mselect_svm_polynomial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mshow_svc_decision_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselect_svm_linear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_svm_polynomial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_svm_radial_basis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Polynomial'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Radial'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-5029bb61a134>\u001b[0m in \u001b[0;36mshow_svc_decision_boundaries\u001b[1;34m(clfs, X, title)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPaired\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Training points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPaired\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feature 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feature 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, hold, data, **kwargs)\u001b[0m\n\u001b[0;32m   3432\u001b[0m                          \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3433\u001b[0m                          \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3434\u001b[1;33m                          edgecolors=edgecolors, data=data, **kwargs)\n\u001b[0m\u001b[0;32m   3435\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3436\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1895\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1896\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1898\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[0;32m   4032\u001b[0m                 \u001b[0moffsets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffsets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4033\u001b[0m                 \u001b[0mtransOffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'transform'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4034\u001b[1;33m                 \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4035\u001b[0m                 )\n\u001b[0;32m   4036\u001b[0m         \u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIdentityTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, paths, sizes, **kwargs)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \"\"\"\n\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m         \u001b[0mCollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_sizes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, edgecolors, facecolors, linewidths, linestyles, antialiaseds, offsets, transOffset, norm, cmap, pickradius, hatch, urls, offset_position, zorder, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hatch_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hatch.color'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_facecolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfacecolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_linewidth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py\u001b[0m in \u001b[0;36mset_facecolor\u001b[1;34m(self, c)\u001b[0m\n\u001b[0;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_facecolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_facecolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_facecolors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py\u001b[0m in \u001b[0;36m_set_facecolor\u001b[1;34m(self, c)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_facecolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Not in cache, or unhashable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0m_colors_full_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrgba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Hirsh Agarwal\\Anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;31m# float)` and `np.array(...).astype(float)` all convert \"0.5\" to 0.5.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;31m# Test dimensionality to reject single floats.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid RGBA argument: {!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m     \u001b[1;31m# Return a tuple to prevent the cached value from being modified.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid RGBA argument: 1"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAEuCAYAAABF6BTJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDxiG6AHD/B8cFEK/niRGsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee36fa4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_x_train = transf_x_train[:, [20, 75]]\n",
    "# Create 3 SVMs\n",
    "select_svm_linear = svm.LinearSVC()\n",
    "select_svm_radial_basis = svm.SVC(kernel='rbf')\n",
    "select_svm_polynomial = svm.SVC(kernel='poly')\n",
    "# Train the SVMs\n",
    "select_svm_linear.fit(select_x_train, y_train)\n",
    "select_svm_radial_basis.fit(select_x_train, y_train)\n",
    "select_svm_polynomial.fit(select_x_train, y_train)\n",
    "show_svc_decision_boundaries([select_svm_linear, select_svm_polynomial, select_svm_radial_basis], select_x_train, ['Linear', 'Polynomial', 'Radial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.21 --- [5 marks] ==========\n",
    "So far we have used default parameters for training the SVM classifiers. Now we want to tune the parameters by using cross-validation. \n",
    "\n",
    "By using the `K-fold` iterator from Question 1.11 and training data only, estimate the classification accuracy of an SVM classifier with RBF kernel, while you vary the penalty parameter `C` in a logarithmic range `np.logspace(-2, 3, 10)`. Set the kernel coefficient parameter `gamma` to `auto` for this question. \n",
    "\n",
    "Plot the mean cross-validated classification accuracy against the regularisation parameter `C` by using a log-scale for the x-axis. Display the highest obtained mean accuracy score and the value of `C` which yielded it. Label axes appropriately. \n",
    "\n",
    "Finally, train a classifier by using the optimal value for this parameter (without using cross-validation at this stage) and report the classification accuracy on the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFJCAYAAAC2OXUDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bedbad91d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test: 97.46774964166268%\n",
      "Accuracy Val: 72.77628032345014%\n"
     ]
    }
   ],
   "source": [
    "c_range = np.logspace(-2, 3, 10)\n",
    "mean_accuracy_values = np.zeros(c_range.shape[0])\n",
    "for i in range(c_range.shape[0]):\n",
    "    rbf_classifier = svm.SVC(kernel='rbf', gamma='auto', C=c_range[i])\n",
    "    current_fold_num = 0\n",
    "    for train, test in kf.split(transf_x_tr):\n",
    "        rbf_classifier = rbf_classifier.fit(transf_x_tr[train], y_tr[train])\n",
    "        pred = rbf_classifier.predict(transf_x_tr[test])\n",
    "        accuracy = (pred == y_tr[test]).sum()/y_tr[test].shape[0]\n",
    "        mean_accuracy_values[i] += accuracy\n",
    "        current_fold_num += 1\n",
    "mean_accuracy_values = mean_accuracy_values/3\n",
    "plt.scatter(np.log(c_range), mean_accuracy_values)\n",
    "plt.show()\n",
    "optimal_c = c_range[np.argmax(mean_accuracy_values)]\n",
    "rbf_optimal = svm.SVC(kernel='rbf', gamma='auto', C=optimal_c)\n",
    "rbf_optimal.fit(transf_x_tr, y_tr)\n",
    "pred = rbf_optimal.predict(transf_x_tr)\n",
    "accuracy_train = (pred == y_tr).sum()/y_tr.shape[0]\n",
    "print(\"Accuracy Test: {}%\".format(accuracy_train*100))\n",
    "pred = rbf_optimal.predict(transf_x_val)\n",
    "accuracy_val = (pred == y_val).sum()/y_val.shape[0]\n",
    "print(\"Accuracy Val: {}%\".format(accuracy_val*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.22 --- [5 marks] ==========\n",
    "Now we turn to the kernel coefficient `gamma` parameter. By using the same procedure as in the previous question, estimate the classification accuracy of an SVM classifier with RBF kernel while you vary the `gamma` parameter in a logarithmic range `logspace(-5, 0, 10)`. Fix the penalty parameter `C=1.0`.\n",
    "\n",
    "Plot the mean cross-validated classification accuracy against the parameter `gamma` by using a log-scale for the x-axis. Display the highest obtained mean accuracy score and the value of `gamma` which yielded it.  Label axes appropriately.\n",
    "\n",
    "Finally, train a classifier by using the optimal value for this parameter (without using cross-validation at this stage) and report the classification accuracy on the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFJCAYAAAChG+XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFaxJREFUeJzt3X9oVff9x/HXzb3mWnejSdpb0FovTdxl8JUtycZoKSHW\nGVbcCuvU3RgyK0rB0bEfSlc6ULJN04gbg/7RtB20DtlGSN2YwvCPLMK2rC0z3Eu5nU2W/hDade2d\nXk1uEm9+3PP9w3k713tzE5vkvO/N8/FXb46HvO8H6/Oe++vjcRzHEQAAMKPM7QEAAMDNiDMAAMYQ\nZwAAjCHOAAAYQ5wBADCGOAMAYIzP7QFuSCRG3R5hTqqqVimZHHd7DHNYl9xYl/xYm9xYl/xKbW2C\nwYq8x7hyniefz+v2CCaxLrmxLvmxNrmxLvktp7UhzgAAGEOcAQAwhjgDAGAMcQYAwBjiDACAMcQZ\nAABjiDMAAMYQZwAAjCHOAAAYQ5wBADCGOAMAYAxxBgDAGOIMAIAxxBkAAGOIMwAAxhBnAACMIc4A\nABhDnAEAMIY4AwBgDHEGAMAY4gwAgDHEGQAAY4gzAADGEGegCKSnZvRhclzpqRm3RwGwBHxuDwAg\nv5lMRt19w4oOJXR5JK3q1X7Vh4OKbNkobxmPrYFSRZwBw7r7htV7/t3s7Usj6ezt1q1ht8YCsMh4\n6A0YlZ6aUXQokfNYdOjfPMUNlDDiDBh1NZXW5ZF0zmPJ0Wu6msp9DEDxI86AUWsCflWv9uc8VlWx\nUmsCuY8BKH7EGTDKv8Kr+nAw57H68B3yr/Au8UQAlgpvCAMMi2zZKOn6a8zJ0Wuqqlip+vAd2Z8D\nKE3EGTDMW1am1q1hbW+q1dVUWmsCfq6YgWWAOANFwL/CqzurVrk9BoAlwmvOAAAYQ5wBADCm4NPa\nmUxG7e3tGhwcVHl5uY4cOaJQKCRJSiQSOnDgQPbPXrhwQQcPHlQkEsl7DgAAmF3BOPf29mpyclLd\n3d2KxWLq7OxUV1eXJCkYDOrkyZOSpGg0qp///Of6xje+Mes5AABgdgXjPDAwoMbGRklSXV2d4vH4\nx/6M4zj6yU9+op/+9Kfyer1zOgdwQ3pqhnc9AzCvYJxTqZQCgUD2ttfr1fT0tHy+j07t6+vTpz/9\nadXU1Mz5nP9VVbVKPl9x/GMZDFa4PYJJltdlZiajF868rlfi7ytxZULBytt076a12vvQ/8nrXdy3\nXlheF7exNrmxLvktl7UpGOdAIKCxsbHs7Uwm87HInj59Wrt3757XOf8rmRyf89BuCgYrlEiMuj2G\nOdbX5de9Qzft7vRhckKn//yWxicmF3V3J+vr4ibWJjfWJb9SW5vZHmgUvGRoaGjQn/70J0lSLBZT\nOPzxf8ji8bgaGhrmdQ6wVNjdCUCxKXjl3NzcrP7+frW0tMhxHHV0dOjMmTMaHx9XJBLR5cuXFQgE\n5PF4Zj0HcMtcdnfiCz4AWFIwzmVlZfrxj398089qa2uz/11dXa3f//73Bc8B3HJjd6dLOQLN7k4A\nLOJLSFDy2N0JQLHhu7WxLLC7E4BiQpyxLLC709LjM+XArSPOWFbY3WnxzWQy6u4bVnQoocsjaVWv\n9qs+HFRky0Z5y3glDZgL4gxgQXX3Dd/0mfJLI+ns7cX8TDlQSngYC2DB8JlyYGEQZwALZi6fKQdQ\nGHEGsGBufKY8Fz5TDswdcQawYPhMObAweEMYgAXFZ8qBT444A1hQfKYc+OSIM4BFwWfKgVvHa84A\nABhDnAEAMIY4AwBgDHEGAMAY4gwAgDHEGQAAY4gzbll6akYfJsfZzAAAFhifc8a85dqv9/7P3aWH\n7tvAfr0AsACIM+Yt1369p//8lsYnJtmvFwAWAJc5mBf26wWAxUecMS/s1wsAi484Y17YrxcAFh9x\nxrywXy8ALD7eEIZ5y7Vf7/2fW6eH7tvg8mQAUBqIM+Yt136969dVKpEYdXs0ACgJxBm3jP16AWBx\n8JozAADGEGcAAIwhzgAAGEOcAQAwhjgDAGAMcQYAwBjiDACAMcQZAABjiDMAAMYQZwAAjCHOAAAY\nQ5wBADCGOAMAYAxxBgDAGOIMAIAxxBkAAGOIMwAAxhBnAACMIc5GpKdm9GFyXOmpGbdHAQC4zOf2\nAMvdTCaj7r5hRYcSujySVvVqv+rDQUW2bJS3jMdOALAcEWeXdfcNq/f8u9nbl0bS2dutW8NujQUA\ncBGXZi5KT80oOpTIeSw69G+e4gaAZYo4u+hqKq3LI+mcx5Kj13Q1lfsYAKC0EWcXrQn4Vb3an/NY\nVcVKrQnkPgYAKG3E2UX+FV7Vh4M5j9WH75B/hXeJJwIAWMAbwlwW2bJR0vXXmJOj11RVsVL14Tuy\nPwcALD/E2WXesjK1bg1re1OtrqbSWhPwc8UMAMsccTbCv8KrO6tWuT0GAMCAgnHOZDJqb2/X4OCg\nysvLdeTIEYVCoezx1157TZ2dnXIcR8FgUMePH5ff79fDDz+sQCAgSVq/fr2eeuqpxbsXAACUkIJx\n7u3t1eTkpLq7uxWLxdTZ2amuri5JkuM4OnTokJ5++mmFQiH19PTovffe01133SXHcXTy5MlFvwMA\nAJSagu/WHhgYUGNjoySprq5O8Xg8e+ztt99WZWWlTpw4oba2Nl25ckU1NTV64403NDExob1792r3\n7t2KxWKLdw8AACgxBa+cU6lU9ulpSfJ6vZqenpbP51MymVQ0GtXhw4e1YcMG7d+/X5s2bVJ1dbX2\n7dunnTt36p133tGjjz6qs2fPyufjJW4AAAopWMtAIKCxsbHs7Uwmk41sZWWlQqGQamtrJUmNjY2K\nx+N65JFHFAqF5PF4dM8996iyslKJREJr167N+3uqqlbJ5yuOdykHgxVuj2AS65Ib65Ifa5Mb65Lf\nclmbgnFuaGjQuXPntG3bNsViMYXDH23GcPfdd2tsbEwXL15UKBTS+fPntWPHDr300ksaGhpSe3u7\nPvjgA6VSKQWDub9s44ZkcvyT35slEAxWKJEYdXsMc1iX3FiX/Fib3FiX/EptbWZ7oFEwzs3Nzerv\n71dLS4scx1FHR4fOnDmj8fFxRSIRHT16VAcPHpTjOKqvr9fmzZs1OTmpJ598Urt27ZLH41FHRwdP\naQMAMEcex3Ect4eQVDSPhkrtkdtCYV1yY13yY21yY13yK7W1me3Kme/WBgDAGOIMAIAxxBkAAGOI\nMwAAxhBnAACMIc4AABhDnAEAMIY4AwBgDHEGAMAY4gwAgDHEGQAAY4gzAADGEGcAAIwhzgAAGEOc\nAQAwhjgDAGAMcQYAwBjiDACAMcQZAABjiDMAAMYQZwAAjCHOAAAYQ5wBADCGOAMAYAxxBgDAGOIM\nAIAxxBkAAGOIMwAAxhBnAACMIc4AABhDnAEAMIY4AwBgDHEGAMAY4gwAgDHEGQAAY4gzAADGEGcA\nAIwhzgAAGEOcAQAwhjgDAGAMcQYAwBjiDACAMcQZAABjiDMAAMYQZwAAjCHOAAAYQ5wBADCGOAMA\nYAxxBgDAGOIMAIAxxBkAAGOIMwAAxhBnAACMIc4AABhDnAEAMIY4AwBgDHEGAMAYX6E/kMlk1N7e\nrsHBQZWXl+vIkSMKhULZ46+99po6OzvlOI6CwaCOHz+uFStWzHoOAADIr2Cce3t7NTk5qe7ubsVi\nMXV2dqqrq0uS5DiODh06pKefflqhUEg9PT167733NDw8nPccAAAwu4JPaw8MDKixsVGSVFdXp3g8\nnj329ttvq7KyUidOnFBbW5uuXLmimpqaWc8BAACzKxjnVCqlQCCQve31ejU9PS1JSiaTikajamtr\n04svvqhXXnlFL7/88qznAACA2RV8WjsQCGhsbCx7O5PJyOe7flplZaVCoZBqa2slSY2NjYrH47Oe\nk09V1Sr5fN5buhNLLRiscHsEk1iX3FiX/Fib3FiX/JbL2hSMc0NDg86dO6dt27YpFospHA5nj919\n990aGxvTxYsXFQqFdP78ee3YsUMbNmzIe04+yeT4J7snSyQYrFAiMer2GOawLrmxLvmxNrmxLvmV\n2trM9kCjYJybm5vV39+vlpYWOY6jjo4OnTlzRuPj44pEIjp69KgOHjwox3FUX1+vzZs3K5PJfOwc\nAAAwNx7HcRy3h5BUNI+GSu2R20JhXXJjXfJjbXJjXfIrtbWZ7cqZLyEBAMAY4gwAgDHEGQAAY4gz\nAADGEGcAAIwhzgAAGEOcAQAwhjgDAGAMcQYAwBjiDACAMcQZAABjiDMAAMYQZwAAjCHOAAAYQ5wB\nADCGOAMAYAxxBgDAGOIMAIAxxBkAAGOIMwAAxhBnAACMIc4AABhDnAEAMIY4AwBgDHEGAMAY4gwA\ngDHEGQAAY4gzAADGEGcAAIwhzgAAGEOcAQAwhjgDAGAMcQYAwBjiDACAMcQZAABjiDMAAMYQZwAA\njCHOAAAYQ5wBADCGOAMAYAxxBgDAGOIMAIAxxBkAAGOIMwAAxhBnAACMIc4AABhDnAEAMIY4AwBg\nDHEGAMAY4gwAgDHEGQAAY4gzAADGEGcAAIwhzgAAGEOcAQAwhjgDAGAMcQYAwBhfoT+QyWTU3t6u\nwcFBlZeX68iRIwqFQtnjJ06cUE9Pj6qrqyVJP/rRj1RTU6OHH35YgUBAkrR+/Xo99dRTi3QXAAAo\nLQXj3Nvbq8nJSXV3dysWi6mzs1NdXV3Z4/F4XMeOHdOmTZuyP0un03IcRydPnlycqQEAKGEFn9Ye\nGBhQY2OjJKmurk7xePym46+//rqef/557dq1S88995wk6Y033tDExIT27t2r3bt3KxaLLcLoAACU\npoJXzqlUKvv0tCR5vV5NT0/L57t+6le+8hW1trYqEAjo29/+ts6dO6d169Zp37592rlzp9555x09\n+uijOnv2bPYcAACQX8FaBgIBjY2NZW9nMplsZB3H0SOPPKKKigpJUlNTk/7+97/r/vvvVygUksfj\n0T333KPKykolEgmtXbs27++pqloln8/7Se/PkggGK9wewSTWJTfWJT/WJjfWJb/lsjYF49zQ0KBz\n585p27ZtisViCofD2WOpVEpf/epX9Yc//EGrVq3Sq6++qu3bt+ull17S0NCQ2tvb9cEHHyiVSikY\nDM76e5LJ8U9+b5ZAMFihRGLU7THMYV1yY13yY21yY13yK7W1me2BRsE4Nzc3q7+/Xy0tLXIcRx0d\nHTpz5ozGx8cViUT0/e9/X7t371Z5ebnuu+8+NTU1aXJyUk8++aR27dolj8ejjo4OntIGAGCOPI7j\nOG4PIaloHg2V2iO3hcK65Ma65Mfa5Ma65FdqazPblTNfQgIAgDHEGQAAY4gzAADGEGcAAIwhzgAA\nGEOcAQAwhjgDAGAMcQYAwBjiDACAMcQZAABjiDMAAMaUZJzTUzP6MDmu9NSM26MAADBvJbVV1Ewm\no+6+YUWHEro8klb1ar/qw0FFtmyUt6wkH4cAAEpQScW5u29Yveffzd6+NJLO3m7dGs53GgAAppTM\n5WR6akbRoUTOY9Ghf/MUNwCgaJRMnK+m0ro8ks55LDl6TVdTuY8BAGBNycR5TcCv6tX+nMeqKlZq\nTSD3MQAArCmZOPtXeFUfDuY8Vh++Q/4V3iWeCACAW1NSbwiLbNko6fprzMnRa6qqWKn68B3ZnwMA\nUAxKKs7esjK1bg1re1OtrqbSWhPwc8UMACg6JRXnG/wrvLqzapXbYwAAcEtK5jVnAABKBXEGAMAY\n4gwAgDHEGQAAY4gzAPwHO9rBipJ8tzYAzAc72sEa/tYBWPZu7Gh3aSQtRx/taNfdN+z2aAUV49U+\nMxfGlTOAZa3Qjnbbm2pNfplRMV7tM/PcEWcAy9pcdrSz+KVGxbh/PTPPnc2HKgCwRIpxR7ti3L+e\nmeeHOANY1opxR7ti3L+emeeHOANY9iJbNmrrF9br9tUrVeaRbl+9Ulu/sN7sjnbFeLXPzPPDa84A\nlr1i29HuxtX+f78WeoPVq31mnh/iDAD/UUw72hXj/vXMPHcex3GcRf0Nc5RIjLo9wpwEgxVFM+tS\nYl1yY13yY21ym++6pKdmiuJq/7/d6sxu/p1ZjHUOBivyHuPKGQCKWDFd7d/AzIXxhjAAAIwhzgAA\nGEOcAQAwhjgDAGAMcQYAwBjiDACAMcQZAABjiDMAAMYQZwAAjCHOAAAYQ5wBADCGOAMAYAxxBgDA\nGOIMAIAxxBkAAGOIMwAAxhBnAACMIc4AABhDnAEAMIY4AwBgDHEGAMAY4gwAgDHEGQAAYzyO4zhu\nDwEAAD7ClTMAAMYQZwAAjCHOAAAYQ5wBADCGOAMAYAxxBgDAGOJ8C9588019/vOfVzqddnsUM0ZH\nR7V//361tbUpEokoGo26PZKrMpmMDh8+rEgkom9+85u6ePGi2yOZMDU1pccff1ytra3asWOH/vjH\nP7o9kjmXLl1SU1OT3nzzTbdHMeO5555TJBLR17/+dfX09Lg9zpLwuT1AsUmlUjp27JjKy8vdHsWU\nF198Uffee6/27Nmjt956SwcPHtTvfvc7t8dyTW9vryYnJ9Xd3a1YLKbOzk51dXW5PZbrTp8+rcrK\nSh0/flxXrlzR1772NX3pS19yeywzpqamdPjwYa1cudLtUcx49dVXFY1G9Zvf/EYTExN64YUX3B5p\nSXDlPA+O4+jQoUM6cOCAbrvtNrfHMWXPnj1qaWmRJM3MzMjv97s8kbsGBgbU2NgoSaqrq1M8Hnd5\nIhsefPBBffe735V0/f8nr9fr8kS2HDt2TC0tLbrzzjvdHsWMv/zlLwqHw3rssce0f/9+bd682e2R\nlgRXznn09PTol7/85U0/W7dunbZt26bPfOYzLk1lQ6616ejo0Gc/+1klEgk9/vjj+uEPf+jSdDak\nUikFAoHsba/Xq+npafl8y/t/uU996lOSrq/Pd77zHX3ve99zeSI7fvvb36q6ulqNjY16/vnn3R7H\njGQyqX/+85969tln9e677+pb3/qWzp49K4/H4/Zoi2p5/0sxi507d2rnzp03/ay5uVmnTp3SqVOn\nlEgktHfvXv3qV79yaUL35FobSRocHNSBAwf0gx/8QF/84hddmMyOQCCgsbGx7O1MJrPsw3zD+++/\nr8cee0ytra166KGH3B7HjFOnTsnj8ejll1/WhQsX9MQTT6irq0vBYNDt0VxVWVmpmpoalZeXq6am\nRn6/X5cvX9btt9/u9miLy8EteeCBB5xr1665PYYZ//jHP5wvf/nLzoULF9wexYSzZ886TzzxhOM4\njhONRp19+/a5PJENiUTCefDBB52//vWvbo9iWltbmzM8POz2GCb09fU5e/bscTKZjPOvf/3L2bp1\nqzM9Pe32WIuOh/JYED/72c80OTmpo0ePSrp+5bic3wDV3Nys/v5+tbS0yHEcdXR0uD2SCc8++6xG\nRkb0zDPP6JlnnpEk/eIXv+ANUMjrgQce0N/+9jft2LFDjuPo8OHDy+K9CuxKBQCAMbxbGwAAY4gz\nAADGEGcAAIwhzgAAGEOcAQAwhjgDAGAMcQYAwBjiDACAMf8PuKuuNgpn39wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee30d9e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test: 0.8910654562828476%\n",
      "Accuracy Val: 0.7268643306379156%\n"
     ]
    }
   ],
   "source": [
    "range_gamma = np.logspace(-5, 0, 10)\n",
    "mean_accuracy_values = np.zeros(range_gamma.shape[0])\n",
    "for i in range(range_gamma.shape[0]):\n",
    "    rbf_classifier = svm.SVC(kernel='rbf', gamma=range_gamma[i], C=1)\n",
    "    current_fold_num = 0\n",
    "    for train, test in kf.split(transf_x_train):\n",
    "        rbf_classifier = rbf_classifier.fit(transf_x_tr[train], y_tr[train])\n",
    "        pred = rbf_classifier.predict(transf_x_tr[test])\n",
    "        accuracy = (pred == y_tr[test]).sum()/y_tr[test].shape[0]\n",
    "        mean_accuracy_values[i] += accuracy\n",
    "        current_fold_num += 1\n",
    "mean_accuracy_values = mean_accuracy_values/3\n",
    "plt.scatter(np.log(c_range), mean_accuracy_values)\n",
    "plt.show()\n",
    "optimal_gamma = range_gamma[np.argmax(mean_accuracy_values)]\n",
    "rbf_optimal = svm.SVC(kernel='rbf', gamma=optimal_gamma, C=1.0)\n",
    "rbf_optimal.fit(transf_x_tr, y_tr)\n",
    "pred = rbf_optimal.predict(transf_x_tr)\n",
    "accuracy_test = (pred == y_tr).sum()/y_tr.shape[0]\n",
    "print(\"Accuracy Test: {}%\".format(accuracy_test))\n",
    "pred = rbf_optimal.predict(transf_x_val)\n",
    "accuracy_val = (pred == y_val).sum()/y_val.shape[0]\n",
    "print(\"Accuracy Val: {}%\".format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.23 --- [7 marks] ==========\n",
    "Now we wish to tune both the `C` and `gamma` parameters simultaneously. To save computational time, we will now constrain the parameter search space. Define a `4 X 4` grid for the two parameters, as follows:\n",
    "* `C`: `np.logspace(-2, 1, 4)`\n",
    "* `gamma`: `np.logspace(-4, -1, 4)`\n",
    "\n",
    "Estimate the mean cross-validated classification accuracy by using training data only and all possible configurations for the two parameters. \n",
    "\n",
    "Use a [heatmap](https://seaborn.github.io/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap) to visualise the mean cross-validated classification accuracy for all `C`-`gamma` pairs. Label axes appropriately and display the values for `C` and `gamma` for the best performing configuration. \n",
    "\n",
    "Finally, by using the optimal configuration, train a classifier (without using cross-validation) and report the classification accuracy on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.46870520783565%\n",
      "Val Accuracy: 72.14734950584007%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAFJCAYAAADnrUZgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXI8wBAC4HmEIAHC9/wZw/OdwRdwv9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee35830b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_space_c = np.logspace(-2, 1, 4)\n",
    "search_space_gamma = np.logspace(-4, -1, 4)\n",
    "mean_accuracy = np.zeros((search_space_c.shape[0], search_space_gamma.shape[0]))\n",
    "for i in range(search_space_c.shape[0]):\n",
    "    for j in range(search_space_gamma.shape[0]):\n",
    "        rbf_optimized = svm.SVC(kernel='rbf', C=search_space_c[i], gamma=search_space_gamma[j])\n",
    "        for train, test in kf.split(transf_x_tr):\n",
    "            rbf_optimized.fit(transf_x_tr[train], y_tr[train])\n",
    "            pred = rbf_optimized.predict(transf_x_tr[test])\n",
    "            accuracy = (pred == y_tr[test]).sum()/y_tr[test].shape[0]\n",
    "            mean_accuracy[i, j] += accuracy\n",
    "mean_accuracy = mean_accuracy/3\n",
    "sns.heatmap(mean_accuracy)\n",
    "# Calculate optimal\n",
    "max_value = np.unravel_index(np.argmax(mean_accuracy), mean_accuracy.shape)\n",
    "best_c = search_space_c[max_value[0]]\n",
    "best_gamma = search_space_gamma[max_value[1]]\n",
    "rbf_optimal = svm.SVC(kernel='rbf', C=best_c, gamma=best_gamma)\n",
    "rbf_optimal.fit(transf_x_tr, y_tr)\n",
    "pred_train = rbf_optimal.predict(transf_x_tr)\n",
    "accuracy_train = (pred_train == y_tr).sum()/y_tr.shape[0]\n",
    "pred_val = rbf_optimal.predict(transf_x_val)\n",
    "accuracy_val = (pred_val == y_val).sum()/y_val.shape[0]\n",
    "print(\"Test Accuracy: {}%\".format(accuracy_train*100))\n",
    "print(\"Val Accuracy: {}%\".format(accuracy_val*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.24 --- [3 marks] ==========\n",
    "Is the classification accuracy on the validation set higher than in previous questions (1.22-1.23)? If not, can you explain why? Can you think of a way of further improving the performance of the classifier? You don't need to implement your suggestion at this stage. Would there be any associated problems with your suggested approach? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the accuracy is actually slightly lower. One issue with this system is that the search resolution is quite low so it might be difficult to find the actual optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.25 --- [5 marks] ==========\n",
    "Now we want to evaluate the performance of an SVM classifier with polynomial kernel. Once again, we will tune the `degree` parameter by using cross-validation (similarly to what we did in Questions 1.21 and 1.22).\n",
    "\n",
    "By using the `K-fold` iterator from Question 1.11 and training data only, estimate the classification accuracy of polynomial SVM classifier, while you vary the `degree` parameter in the range `np.arange(1,8)`. \n",
    "\n",
    "Plot the mean cross-validated classification accuracy against the polynomial degree. Display the highest obtained mean accuracy score and the value of the `degree` parameter which yielded it. Label axes appropriately. \n",
    "\n",
    "Finally, train a classifier by using the optimal value for this parameter (without using cross-validation at this stage) and report the classification accuracy on the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFJCAYAAAC2OXUDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bee30819b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 78.35642618251313%\n",
      "Validation Accuracy: 68.91284815813118%\n"
     ]
    }
   ],
   "source": [
    "search_space_degree = np.arange(1,8)\n",
    "mean_accuracy = np.zeros(search_space_degree.shape[0])\n",
    "for i in range(search_space_degree.shape[0]):\n",
    "    poly_svm = svm.SVC(kernel='poly', degree=search_space_degree[i])\n",
    "    for train, test in kf.split(transf_x_tr):\n",
    "        poly_svm.fit(transf_x_tr[train], y_tr[train])\n",
    "        pred = poly_svm.predict(transf_x_tr[test])\n",
    "        accuracy = (pred == y_tr[test]).sum()/y_tr[test].shape[0]\n",
    "        mean_accuracy[i] += accuracy\n",
    "mean_accuracy = mean_accuracy/3\n",
    "plt.scatter(search_space_degree, mean_accuracy)\n",
    "plt.show()\n",
    "poly_svm = svm.SVC(kernel='poly', degree=search_space_degree[np.argmax(mean_accuracy)])\n",
    "poly_svm.fit(transf_x_tr, y_tr)\n",
    "pred_training = poly_svm.predict(transf_x_tr)\n",
    "accuracy_training = (pred_training == y_tr).sum()/y_tr.shape[0]\n",
    "pred = poly_svm.predict(transf_x_val)\n",
    "accuracy = (pred == y_val).sum()/y_val.shape[0]\n",
    "print('Training Accuracy: {}%'.format(accuracy_training*100))\n",
    "print('Validation Accuracy: {}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.26 --- [4 marks] ==========\n",
    "\n",
    "You might have noticted that so far, we have used cross-validation for optimising the various tuning parameters (e.g. regularisation parameter in logistic regression, SVM kernel parameters) rather than hold-out validation, although we did have access to a validation set. Why do you think this is a good/bad idea? Give one advantage and one disadvantage of the two different approaches. Which one would you trust more in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.27 --- [6 marks] ==========\n",
    "\n",
    "Reload the full training and validation data that contain all indicator variables for all object categories. Remove the `imgId` attribute but keep all of the class indicator variables in the dataset this time. Your training features should include all attributes except `is_person` which should be your target variable. \n",
    "\n",
    "Once again, use a [StandardScaler](http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.StandardScaler.html) to standardise your training and validation features. Then train a Random Forest Classifier by using the entropy `criterion`, 500 `n_estimators`, and also set the `random_state` to 31. Report the classification accuracy on the training and validation sets.\n",
    "\n",
    "Similarly to what we did in Question 1.18, order the features by decreasing importance and display the 50 most important features. \n",
    "\n",
    "Finally, answer the following questions:\n",
    "* What do you notice by looking at the list of the best 50 features?\n",
    "* How does the performance differ with respect to the case when the additional class indicator variables are not present (Question 1.16)? Relate your observations to the observed feature ranking.\n",
    "* Would it be easy to make use of the results in practice? Briey explain your reasoning.\n",
    "\n",
    "*(Hint: you might want to look at some of the [images](http://www.inf.ed.ac.uk/teaching/courses/iaml/2014/assts/asst3/images.html) to justify your explanations.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0%\n",
      "Validation Accuracy: 71.15902964959568%\n",
      "[('is_car', 0.0093786604641707457), ('is_bicycle', 0.0087309056175232311), ('dim20', 0.008030268242118669), ('dim50', 0.0059245937795918326), ('dim359', 0.005463270730147453), ('dim3', 0.0053254635730240014), ('dim75', 0.0052711094276990432), ('dim397', 0.0052542912934713882), ('dim282', 0.0049825214687269567), ('dim478', 0.0049417709169217588), ('dim262', 0.0047338405389478009), ('dim342', 0.0046091911270877816), ('dim422', 0.0044332941459177136), ('dim484', 0.0042446900093632139), ('dim253', 0.004188582305401224), ('dim460', 0.0041575039787235292), ('dim221', 0.0041343771358179392), ('dim355', 0.0039138875860249295), ('dim73', 0.0038603367481714335), ('dim287', 0.0038503656836912049), ('dim213', 0.0037915869326592007), ('dim347', 0.0037810959049539117), ('dim329', 0.003712548531421811), ('dim216', 0.0036834267865541752), ('dim482', 0.0036327747075583202), ('dim89', 0.0036258903666367714), ('dim273', 0.0035970303119618125), ('dim346', 0.0035330997133573776), ('dim16', 0.0034476083156718068), ('dim426', 0.0033773881272774462), ('dim34', 0.0033058220739703228), ('dim325', 0.0032983574618096344), ('dim72', 0.0032667142601207795), ('dim311', 0.0031933077985880041), ('dim47', 0.0031604102046429577), ('dim24', 0.0031551587118597022), ('dim314', 0.0031460858483183824), ('dim321', 0.0031284246543865123), ('dim328', 0.0031011368033754525), ('dim499', 0.0030852198752739659), ('dim133', 0.0030344063454050578), ('dim205', 0.0030165591431819709), ('dim500', 0.0029979035592304468), ('dim95', 0.0029678971654708108), ('dim352', 0.0029649800394587265), ('dim441', 0.0029519998680364382), ('dim76', 0.0029428984879409231), ('dim439', 0.002927260769000772), ('dim170', 0.0029103625012924781), ('dim153', 0.0028770288987194012)]\n"
     ]
    }
   ],
   "source": [
    "# Should the other binary attributes be removed?\n",
    "# Import the data\n",
    "train_A2 = pd.read_csv('datasets/train_images_partA.csv')\n",
    "valid_A2 = pd.read_csv('datasets/valid_images_partA.csv')\n",
    "train_A2 = train_A2.drop('imgId', axis=1)\n",
    "valid_A2 = valid_A2.drop('imgId', axis=1)\n",
    "A2_train_x = train_A2.drop('is_person', axis=1)\n",
    "A2_train_y = train_A2['is_person']\n",
    "A2_val_x = valid_A2.drop('is_person', axis=1)\n",
    "A2_val_y = valid_A2['is_person']\n",
    "sc2 = StandardScaler()\n",
    "sc2 = sc2.fit(A2_train_x)\n",
    "A2_train_x_tr = sc2.transform(A2_train_x)\n",
    "A2_val_x_tr = sc2.transform(A2_val_x)\n",
    "rf = RandomForestClassifier(criterion='entropy', n_estimators=500, random_state=31)\n",
    "rf.fit(A2_train_x_tr, A2_train_y)\n",
    "accuracy_train = rf.score(A2_train_x_tr, A2_train_y)\n",
    "accuracy_val = rf.score(A2_val_x_tr, A2_val_y)\n",
    "print(\"Training Accuracy: {}%\".format(accuracy_train*100))\n",
    "print(\"Validation Accuracy: {}%\".format(accuracy_val*100))\n",
    "feature_importance = zip(attribute_names, rf.feature_importances_, )\n",
    "feature_importance = sorted(feature_importance, key=itemgetter(1))[::-1]\n",
    "top_50_features = feature_importance[:50]\n",
    "print(top_50_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini challenge [30%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Important: You are allowed to write up to a maximum of 600 words in this part of the assignment. The thoroughness of the exploration and the quality of the resulting discussion is just as important as the final classification performance of your chosen method(s) and credit will be divided accordingly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final part of the assignment we will have a mini object-recognition challenge. Using the data provided you are asked to find the best classiffier for the person/no person classification task. You can apply any preprocessing steps to the data that you think fit and employ any classifier you like (with the provison that you can explain what the classifier is/preprocessing steps are doing). You can also employ any lessons learnt during the course, either from previous Assignments, the Labs or the lecture material to try and squeeze out as much performance as you possibly can. The only restriction is that all steps must be performed in `Python` by using the `numpy`, `pandas` and `sklearn` packages. You can also make use of `matplotlib` and `seaborn` for visualisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** The classification performance metric that we will use for this part is the *cross-entropy* or *logarithmic loss* (see the labs). You should familiarise yourself with the metric by reading the `sklearn` [user guide](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss) and [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss). To estimate this metric you will need to evaluate probability outputs, as opposed to discrete predictions which we have used so far to compute classification accuracies. Most models in `sklearn` implement a `predict_proba()` method which returns the probabilities for each class. For instance, if your test set consists of `N` datapoints and there are `K` classes, the method will return a `N` x `K` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with three new data sets: a training set (`train_images_partB.csv`), a validation set (`valid_images_partB.csv`), and a test set (`test_images_partB.csv`). You must use the former two for training and evaluating your models (as you see fit). Once you have chosen your favourite model (and pre-processing steps) you should apply it to the test set (for which no labels are provided). Estimate the posterior proabilities for the data points in the test set and submit your results as part of your answer. Your results will be evaluated in terms of the logarithmic loss metric. You also need to submit a brief description of the approaches you considered, your suggested final approach, and a short explanation of why you chose it. The thoroughness of the exploration and the quality of the resulting discussion is just as important as the final score of your chosen method(s) and credit will be divided accordingly.\n",
    "\n",
    "*Hint: Feature engineering, feature combination, model combination and model parameter optimization can significantly improve performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to submit your results**: Store the estimated probabilities for the data points in the test set in a 2D numpy array. Then execute the provided cell at the end of this notebook which uses a provided `save_predictions` function to export your results into a `.txt` file (the function will return an error if the provided array has not the right shape). The `.txt` file will be saved where your notebook lives. Submit this file along with your notebook as detailed at the top of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here (max. 600 words)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to export your results\n",
    "from numpy import savetxt\n",
    "def save_predictions(pred_proba):\n",
    "    if pred_proba.shape != (1114,2):\n",
    "        raise ValueError('Predicted probabilities array is not the right shape.')\n",
    "    \n",
    "    savetxt('assignment_3_predictions.txt', pred_proba)\n",
    "\n",
    "# You need to replace \"test_images_partB_pred_proba\"\n",
    "# with the name of the array which contains the probability \n",
    "# estimates for the data in the test set.\n",
    "save_predictions(test_images_partB_pred_proba) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information about visual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual words are based on [Scale-invariant feature transforms (SIFT)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). SIFT features are essentially local orientation histograms and capture the properties of small image regions. They possess attractive invariance properties which make them well suited for our task (you can read more about SIFT features in [D.Lowe, IJCV 60(2):91- 110, 2004](http://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94), but the details don't matter for the purpose of this assignment). Each SIFT feature is a 128 dimensional vector. From each image many SIFT features are extracted, typically > 2500 per image (features are extracted at regular intervals using a 15 pixel grid and at 4 different scales). To obtain visual words a representative subset of all extracted SIFT features from all images is chosen and clustered with k-means using 500 centres (such use of the k-means algorithm will be discussed in detail during the lecture). These 500 cluster centres form our visual words. The representation of a single image is obtained by first assigning each SIFT feature extracted from the image to the appropriate cluster (i.e. we determine the visual word corresponding to each feature by picking the closest cluster centre). We then count the number of features from that image assigned to each cluster (i.e. we determine how often each visual word is present in the image). This results in a 500 dimensional count vector for each image (one dimension for each visual word). The normalized version of this count vector gives the final representation of the image (normalized means that we divide the count vector by the total number of visual words in the image, i.e. the normalized counts sum to 1 for each image)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
